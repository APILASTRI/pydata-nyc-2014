{
 "metadata": {
  "name": "",
  "signature": "sha256:5abce705b9f8d1df0da77bd014cfd0a6552af20f704c714ac723aeabb36b72d7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we learn something so far, that is machine learning is not only applying an algorithm and get the predictions. It has quite a lot of different and moving parts for a given problem. Steps(feature extraction, feature selection, classifier, evaluation) follows a sequential order, though. Would it be perfect if we could wrap all of the steps in one object and then do the parameter search(i.e. grid parameter search) for cross validation in that object. Further, if we have two estimators in the __pipeline__(say we apply PCA to reduce dimension in the input and then apply SVM), we would need only once to use the `fit` function in the estimator. Pipeline automatically applies the correct steps for you to get the correct output at the end of the pipeline. Still not convinced? What if I say, serializing one `pipeline` instead of serializing `vectorizer`, `feature_selector` and `classifier` separately and then deploying into production makes much easier.(More on to this in the next notebook) This was a quick win for the pipeline. Let's see how one might use it in text classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import os\n",
      "from sklearn import cross_validation\n",
      "from sklearn import ensemble\n",
      "from sklearn.feature_extraction import text\n",
      "from sklearn import feature_extraction\n",
      "from sklearn import feature_selection\n",
      "from sklearn import metrics\n",
      "from sklearn import naive_bayes\n",
      "from sklearn import pipeline\n",
      "from sklearn import ensemble\n",
      "\n",
      "_DATA_DIR = 'data'\n",
      "_NYT_DATA_PATH = os.path.join(_DATA_DIR, 'nyt_title_data.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(_NYT_DATA_PATH) as nyt:\n",
      "    nyt_data = []\n",
      "    nyt_labels = []\n",
      "    csv_reader = csv.reader(nyt)\n",
      "    for line in csv_reader:\n",
      "      nyt_labels.append(int(line[0]))\n",
      "      nyt_data.append(line[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.array([''.join(el) for el in nyt_data])\n",
      "y = np.array([el for el in nyt_labels])\n",
      "\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y)\n",
      "\n",
      "vectorizer = (min_df=2, \n",
      " ngram_range=(1, 2), \n",
      " stop_words='english', \n",
      " strip_accents='unicode', \n",
      " norm='l2')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Let's Create our pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe = pipeline.Pipeline([(\"vectorizer\", vectorizer), (\"rf\", ensemble.RandomForestClassifier)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipe.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unbound method fit() must be called with RandomForestClassifier instance as first argument (got csr_matrix instance instead)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-1d40af031c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/bugra/Dev/Library/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: unbound method fit() must be called with RandomForestClassifier instance as first argument (got csr_matrix instance instead)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functions that are applicable to estimators are also applicable to Pipelines. That is one of the most powerful premise of the pipeline after all. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.accuracy_score(pipe.predict(X_test), y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "0.64695009242144175"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Upto here, you may start convincing yourself how pipeline would be much better and useful and easier than applying each separate component in the machine learning pipeline. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Grid Search in Pipeline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One might apply grid search to the pipeline similar to what we did on the estimator as well. Then one may ask, how do wepass parameters for vectorizer, featur selector and classifier. In the grid search of an estimator, this would be easy as you could pass a `dictionary` which has the keys for the parameters and the parameters as lists that you want to optimize. However, the things in `pipeline` is not that straightforward. First, what if two estimators share the same parameter name and you want to give different values in the search space. What if you do not want to pass any list of parameter to one and pass to the other one? In order to handle this ambiguity, `pipeline` accepts parameters in the form of `{name}__{parameter}` in the dictionary where the `{name}` is the name of the step that you are passing to the pipeline and the parameter is the parameter name that you want to optimize in that step."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h4> Why double leading underscore?</h4>\n",
      "> <b>__double_leading_underscore</b>: when naming a class attribute, invokes name mangling (inside class FooBar, __boo becomes _FooBar__boo; ) \n",
      "\n",
      "From [PEP 8](http://legacy.python.org/dev/peps/pep-0008/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://igorsobreira.com/2010/09/16/difference-between-one-underline-and-two-underlines-in-python.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- http://scikit-learn.org/stable/modules/grid_search.html\n",
      "- http://scikit-learn.org/stable/auto_examples/feature_stacker.html#example-feature-stacker-py\n",
      "- http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion\n",
      "- http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html\n",
      "- http://scikit-learn.org/stable/modules/pipeline.html\n",
      "- "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the pipeline, we have only two steps: `vectorizer` and `svm`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = dict(vectorizer__=[2, 5, 10],\n",
      "              rf__n_estimators=[10, 50, 100, 1000],\n",
      "              rf__criterion=['gini', 'entropy'],\n",
      "              rf__min_samples_split=[2, 4, 6],\n",
      "              rf__)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}