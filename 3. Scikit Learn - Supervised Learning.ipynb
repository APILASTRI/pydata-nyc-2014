{
 "metadata": {
  "name": "",
  "signature": "sha256:554131aa41a118f5989087c601a9632f0cfc64f69298696087240c55de02ed11"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Supervised Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supervised Learning is the most commonly referred/mentioned/researched/published subfield in the machine learning. Unsurprisingly, all of the predictive algorithms fall in to this category. Its premise is quite simple, when you provide a labeled dataset to the algorithm(training set), it will predict unseen dataset's (test set) output variables, whether be it a discrete label(classification) or a continuous value(regression). The learning function(regressor or classifier) will first `fit` to the training set and then try to `predict` the dataset that it has never seen, which Scikit-Learn follows these two steps literally. More on this later. First, what is the problem that we are trying to solve?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dataset is from a telephone service provider where they have the service usage(international plan, voicemail plan, usage in daytime, usage in evenings and nights and so on) and basic demographic information(state and area code) of the user. For labels, I have a single data point whether the customer is churned out or not. \n",
      "Dataset is from a competititon that CrowdAnalytics did I believe and the original contest text is given in below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Original Dataset Explanation\n",
      "    Churn (loss of customers to competition) is a problem for telecom companies because it is more expensive to acquire a new customer than to keep your existing one from leaving. This contest is about enabling churn reduction using analytics.\n",
      "\n",
      "    The Business Pain:\n",
      "\n",
      "    Most telecom companies suffer from voluntary churn. Churn rate has strong impact on the life time value of the customer because it affects the length of service and the future revenue of the company. For example if a company has 25% churn rate then the average customer lifetime is 4 years; similarly a company with a churn rate of 50%, has an average customer lifetime of 2 years. It is estimated that 75 percent of the 17 to 20 million subscribers signing up with a new wireless carrier every year are coming from another wireless provider, which means they are churners. Telecom companies spend hundreds of dollars to acquire a new customer and when that customer leaves, the company not only loses the future revenue from that customer but also the resources spend to acquire that customer. Churn erodes profitability.\n",
      "\n",
      "    Steps that have been adopted by telecom companies so far:\n",
      "\n",
      "    Telecom companies have used two approaches to address churn - (a) Untargeted approach and (b) Targeted approach. The untargeted approach relies on superior product and mass advertising to increase brand loyalty and thus retain customers. The targeted approach relies on identifying customers who are likely to churn, and  provide suitable intervention to encourage them to stay.\n",
      "\n",
      "    Role of predictive modeling:\n",
      "\n",
      "    In the targeted approach the company tries to identify in advance customers who are likely to churn. The company then targets those customers with special programs or incentives. This approach can bring in huge loss for a company, if churn predictions are inaccurate, because then firms are wasting incentive money on customers who would have stayed anyway. There are numerous predictive modeling techniques for predicting customer churn. These vary in terms of statistical technique (e.g., neural nets versus logistic regression versus survival analysis), and variable selection method (e.g., theory versus stepwise selection).\n",
      "\n",
      "    Objective of this Contest:\n",
      "\n",
      "    The objective of this contest is to predict customer churn. We are providing you a public dataset that has customer usage pattern and if the customer has churned or not. We expect you to develop an algorithm to predict the churn score based on usage pattern. The predictors provided are as follows:\n",
      "\n",
      "    account length\n",
      "    international plan\n",
      "    voice mail plan\n",
      "    number of voice mail messages\n",
      "    total day minutes used\n",
      "    day calls made\n",
      "    total day charge\n",
      "    total evening minutes\n",
      "    total evening calls\n",
      "    total evening charge\n",
      "    total night minutes\n",
      "    total night calls\n",
      "    total night charge\n",
      "    total international minutes used\n",
      "    total international calls made\n",
      "    total international charge\n",
      "    number customer service calls made"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Problem Formulation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Customer Churn: losing/attrition of the customers from the company. Especially, the industries that the user acquisition is costly, it is crucially important for one company to reduce and ideally make the customer churn to 0 to sustain their recurring revenue. If you consider customer retention is always cheaper than customer acquisition and generally depends on the data of the user(usage of the service or product), it poses a great/exciting/hard problem for machine learning. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Explanation of Walkthrough"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, I will first walk through a classification example where I try to predict if a customer churns or not based on a variety of attributes that she has. Customer churn prevention allows companies to react preemptively to the customer and then try to provide a better experience to her. This has two advantages to the company; product gets better with the feedback that company received from the customer and as customer continues to use the product, company would not lose the customer. Customer Relation Management(CRM) tools that has the user experience and history of the user may provide a profile for the user. \n",
      "\n",
      "In order to do so, I will first do preprocessing on the attirbutes as most of the attributes vary a lot both intra-class and inter-class changes. Then, I will try to compare different classifiers. Since the dataset has an unbalancad classes(not churned users are 6 times of churned users), it provides also a nice case for different metrics usage as well and why classification accuracy may not be a good metric to optimize in this case. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Binary Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This labeling schema is arguably the most common classification schema which is called binary classification as I have only two classes to predict(churn or not). Even some of the multi-class classification problems could be expressed as a binary classification schema(one vs other classes). Therefore, it is a powerful/basic classification schema that you would use approximately 83%(put some random number > 50) of your machine learning problems."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline \n",
      "\n",
      "import matplotlib as mlp\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import os\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "\n",
      "from sklearn import cross_validation\n",
      "from sklearn import tree\n",
      "from sklearn import svm\n",
      "from sklearn import ensemble\n",
      "from sklearn import neighbors\n",
      "from sklearn import linear_model\n",
      "from sklearn import metrics\n",
      "from sklearn import preprocessing\n",
      "\n",
      "plt.style.use('fivethirtyeight') # Good looking plots\n",
      "pd.set_option('display.max_columns', None) # Display any number of columns\n",
      "\n",
      "_DATA_DIR = 'data'\n",
      "_CHURN_DATA_PATH = os.path.join(_DATA_DIR, 'churn.csv')\n",
      "\n",
      "import seaborn as sns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(_CHURN_DATA_PATH)\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>State</th>\n",
        "      <th>Account Length</th>\n",
        "      <th>Area Code</th>\n",
        "      <th>Phone</th>\n",
        "      <th>Int'l Plan</th>\n",
        "      <th>VMail Plan</th>\n",
        "      <th>VMail Message</th>\n",
        "      <th>Day Mins</th>\n",
        "      <th>Day Calls</th>\n",
        "      <th>Day Charge</th>\n",
        "      <th>Eve Mins</th>\n",
        "      <th>Eve Calls</th>\n",
        "      <th>Eve Charge</th>\n",
        "      <th>Night Mins</th>\n",
        "      <th>Night Calls</th>\n",
        "      <th>Night Charge</th>\n",
        "      <th>Intl Mins</th>\n",
        "      <th>Intl Calls</th>\n",
        "      <th>Intl Charge</th>\n",
        "      <th>CustServ Calls</th>\n",
        "      <th>Churn?</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> KS</td>\n",
        "      <td> 128</td>\n",
        "      <td> 415</td>\n",
        "      <td> 382-4657</td>\n",
        "      <td>  no</td>\n",
        "      <td> yes</td>\n",
        "      <td> 25</td>\n",
        "      <td> 265.1</td>\n",
        "      <td> 110</td>\n",
        "      <td> 45.07</td>\n",
        "      <td> 197.4</td>\n",
        "      <td>  99</td>\n",
        "      <td> 16.78</td>\n",
        "      <td> 244.7</td>\n",
        "      <td>  91</td>\n",
        "      <td> 11.01</td>\n",
        "      <td> 10.0</td>\n",
        "      <td> 3</td>\n",
        "      <td> 2.70</td>\n",
        "      <td> 1</td>\n",
        "      <td> False.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> OH</td>\n",
        "      <td> 107</td>\n",
        "      <td> 415</td>\n",
        "      <td> 371-7191</td>\n",
        "      <td>  no</td>\n",
        "      <td> yes</td>\n",
        "      <td> 26</td>\n",
        "      <td> 161.6</td>\n",
        "      <td> 123</td>\n",
        "      <td> 27.47</td>\n",
        "      <td> 195.5</td>\n",
        "      <td> 103</td>\n",
        "      <td> 16.62</td>\n",
        "      <td> 254.4</td>\n",
        "      <td> 103</td>\n",
        "      <td> 11.45</td>\n",
        "      <td> 13.7</td>\n",
        "      <td> 3</td>\n",
        "      <td> 3.70</td>\n",
        "      <td> 1</td>\n",
        "      <td> False.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> NJ</td>\n",
        "      <td> 137</td>\n",
        "      <td> 415</td>\n",
        "      <td> 358-1921</td>\n",
        "      <td>  no</td>\n",
        "      <td>  no</td>\n",
        "      <td>  0</td>\n",
        "      <td> 243.4</td>\n",
        "      <td> 114</td>\n",
        "      <td> 41.38</td>\n",
        "      <td> 121.2</td>\n",
        "      <td> 110</td>\n",
        "      <td> 10.30</td>\n",
        "      <td> 162.6</td>\n",
        "      <td> 104</td>\n",
        "      <td>  7.32</td>\n",
        "      <td> 12.2</td>\n",
        "      <td> 5</td>\n",
        "      <td> 3.29</td>\n",
        "      <td> 0</td>\n",
        "      <td> False.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> OH</td>\n",
        "      <td>  84</td>\n",
        "      <td> 408</td>\n",
        "      <td> 375-9999</td>\n",
        "      <td> yes</td>\n",
        "      <td>  no</td>\n",
        "      <td>  0</td>\n",
        "      <td> 299.4</td>\n",
        "      <td>  71</td>\n",
        "      <td> 50.90</td>\n",
        "      <td>  61.9</td>\n",
        "      <td>  88</td>\n",
        "      <td>  5.26</td>\n",
        "      <td> 196.9</td>\n",
        "      <td>  89</td>\n",
        "      <td>  8.86</td>\n",
        "      <td>  6.6</td>\n",
        "      <td> 7</td>\n",
        "      <td> 1.78</td>\n",
        "      <td> 2</td>\n",
        "      <td> False.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> OK</td>\n",
        "      <td>  75</td>\n",
        "      <td> 415</td>\n",
        "      <td> 330-6626</td>\n",
        "      <td> yes</td>\n",
        "      <td>  no</td>\n",
        "      <td>  0</td>\n",
        "      <td> 166.7</td>\n",
        "      <td> 113</td>\n",
        "      <td> 28.34</td>\n",
        "      <td> 148.3</td>\n",
        "      <td> 122</td>\n",
        "      <td> 12.61</td>\n",
        "      <td> 186.9</td>\n",
        "      <td> 121</td>\n",
        "      <td>  8.41</td>\n",
        "      <td> 10.1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 2.73</td>\n",
        "      <td> 3</td>\n",
        "      <td> False.</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "  State  Account Length  Area Code     Phone Int'l Plan VMail Plan  \\\n",
        "0    KS             128        415  382-4657         no        yes   \n",
        "1    OH             107        415  371-7191         no        yes   \n",
        "2    NJ             137        415  358-1921         no         no   \n",
        "3    OH              84        408  375-9999        yes         no   \n",
        "4    OK              75        415  330-6626        yes         no   \n",
        "\n",
        "   VMail Message  Day Mins  Day Calls  Day Charge  Eve Mins  Eve Calls  \\\n",
        "0             25     265.1        110       45.07     197.4         99   \n",
        "1             26     161.6        123       27.47     195.5        103   \n",
        "2              0     243.4        114       41.38     121.2        110   \n",
        "3              0     299.4         71       50.90      61.9         88   \n",
        "4              0     166.7        113       28.34     148.3        122   \n",
        "\n",
        "   Eve Charge  Night Mins  Night Calls  Night Charge  Intl Mins  Intl Calls  \\\n",
        "0       16.78       244.7           91         11.01       10.0           3   \n",
        "1       16.62       254.4          103         11.45       13.7           3   \n",
        "2       10.30       162.6          104          7.32       12.2           5   \n",
        "3        5.26       196.9           89          8.86        6.6           7   \n",
        "4       12.61       186.9          121          8.41       10.1           3   \n",
        "\n",
        "   Intl Charge  CustServ Calls  Churn?  \n",
        "0         2.70               1  False.  \n",
        "1         3.70               1  False.  \n",
        "2         3.29               0  False.  \n",
        "3         1.78               2  False.  \n",
        "4         2.73               3  False.  "
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Discreet value integer encoder\n",
      "label_encoder = preprocessing.LabelEncoder()\n",
      "\n",
      "# Get the Labels as integers\n",
      "df['Churn'] = df['Churn?'] == 'True.'\n",
      "y = df['Churn'].as_matrix().astype(np.int)\n",
      "\n",
      "# State is string and we want discre integer values\n",
      "df['State'] = label_encoder.fit_transform(df['State'])\n",
      "\n",
      "# Drop the redundant columns from dataframe\n",
      "df.drop(['Area Code','Phone','Churn?', 'Churn'], axis=1, inplace=True)\n",
      "\n",
      "# Get the features as integers similar to what we did for labels(targets)\n",
      "df[[\"Int'l Plan\",\"VMail Plan\"]] = df[[\"Int'l Plan\",\"VMail Plan\"]] == 'yes'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('There are {} instances for churn class and {} instances for not-churn classes.'.format(y.sum(), y.shape[0] - y.sum()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 483 instances for churn class and 2850 instances for not-churn classes.\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Ratio of churn class over all instances: {:.2f}'.format(float(y.sum()) / y.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ratio of churn class over all instances: 0.14\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kind of unbalanced data, do not you think? I will try to handle this unbalance in the cross validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>State</th>\n",
        "      <th>Account Length</th>\n",
        "      <th>Int'l Plan</th>\n",
        "      <th>VMail Plan</th>\n",
        "      <th>VMail Message</th>\n",
        "      <th>Day Mins</th>\n",
        "      <th>Day Calls</th>\n",
        "      <th>Day Charge</th>\n",
        "      <th>Eve Mins</th>\n",
        "      <th>Eve Calls</th>\n",
        "      <th>Eve Charge</th>\n",
        "      <th>Night Mins</th>\n",
        "      <th>Night Calls</th>\n",
        "      <th>Night Charge</th>\n",
        "      <th>Intl Mins</th>\n",
        "      <th>Intl Calls</th>\n",
        "      <th>Intl Charge</th>\n",
        "      <th>CustServ Calls</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 16</td>\n",
        "      <td> 128</td>\n",
        "      <td> False</td>\n",
        "      <td>  True</td>\n",
        "      <td> 25</td>\n",
        "      <td> 265.1</td>\n",
        "      <td> 110</td>\n",
        "      <td> 45.07</td>\n",
        "      <td> 197.4</td>\n",
        "      <td>  99</td>\n",
        "      <td> 16.78</td>\n",
        "      <td> 244.7</td>\n",
        "      <td>  91</td>\n",
        "      <td> 11.01</td>\n",
        "      <td> 10.0</td>\n",
        "      <td> 3</td>\n",
        "      <td> 2.70</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 35</td>\n",
        "      <td> 107</td>\n",
        "      <td> False</td>\n",
        "      <td>  True</td>\n",
        "      <td> 26</td>\n",
        "      <td> 161.6</td>\n",
        "      <td> 123</td>\n",
        "      <td> 27.47</td>\n",
        "      <td> 195.5</td>\n",
        "      <td> 103</td>\n",
        "      <td> 16.62</td>\n",
        "      <td> 254.4</td>\n",
        "      <td> 103</td>\n",
        "      <td> 11.45</td>\n",
        "      <td> 13.7</td>\n",
        "      <td> 3</td>\n",
        "      <td> 3.70</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 31</td>\n",
        "      <td> 137</td>\n",
        "      <td> False</td>\n",
        "      <td> False</td>\n",
        "      <td>  0</td>\n",
        "      <td> 243.4</td>\n",
        "      <td> 114</td>\n",
        "      <td> 41.38</td>\n",
        "      <td> 121.2</td>\n",
        "      <td> 110</td>\n",
        "      <td> 10.30</td>\n",
        "      <td> 162.6</td>\n",
        "      <td> 104</td>\n",
        "      <td>  7.32</td>\n",
        "      <td> 12.2</td>\n",
        "      <td> 5</td>\n",
        "      <td> 3.29</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 35</td>\n",
        "      <td>  84</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td>  0</td>\n",
        "      <td> 299.4</td>\n",
        "      <td>  71</td>\n",
        "      <td> 50.90</td>\n",
        "      <td>  61.9</td>\n",
        "      <td>  88</td>\n",
        "      <td>  5.26</td>\n",
        "      <td> 196.9</td>\n",
        "      <td>  89</td>\n",
        "      <td>  8.86</td>\n",
        "      <td>  6.6</td>\n",
        "      <td> 7</td>\n",
        "      <td> 1.78</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 36</td>\n",
        "      <td>  75</td>\n",
        "      <td>  True</td>\n",
        "      <td> False</td>\n",
        "      <td>  0</td>\n",
        "      <td> 166.7</td>\n",
        "      <td> 113</td>\n",
        "      <td> 28.34</td>\n",
        "      <td> 148.3</td>\n",
        "      <td> 122</td>\n",
        "      <td> 12.61</td>\n",
        "      <td> 186.9</td>\n",
        "      <td> 121</td>\n",
        "      <td>  8.41</td>\n",
        "      <td> 10.1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 2.73</td>\n",
        "      <td> 3</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "   State  Account Length Int'l Plan VMail Plan  VMail Message  Day Mins  \\\n",
        "0     16             128      False       True             25     265.1   \n",
        "1     35             107      False       True             26     161.6   \n",
        "2     31             137      False      False              0     243.4   \n",
        "3     35              84       True      False              0     299.4   \n",
        "4     36              75       True      False              0     166.7   \n",
        "\n",
        "   Day Calls  Day Charge  Eve Mins  Eve Calls  Eve Charge  Night Mins  \\\n",
        "0        110       45.07     197.4         99       16.78       244.7   \n",
        "1        123       27.47     195.5        103       16.62       254.4   \n",
        "2        114       41.38     121.2        110       10.30       162.6   \n",
        "3         71       50.90      61.9         88        5.26       196.9   \n",
        "4        113       28.34     148.3        122       12.61       186.9   \n",
        "\n",
        "   Night Calls  Night Charge  Intl Mins  Intl Calls  Intl Charge  \\\n",
        "0           91         11.01       10.0           3         2.70   \n",
        "1          103         11.45       13.7           3         3.70   \n",
        "2          104          7.32       12.2           5         3.29   \n",
        "3           89          8.86        6.6           7         1.78   \n",
        "4          121          8.41       10.1           3         2.73   \n",
        "\n",
        "   CustServ Calls  \n",
        "0               1  \n",
        "1               1  \n",
        "2               0  \n",
        "3               2  \n",
        "4               3  "
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After preprocessing, dataframe is ready to be represented as matrix that is amenable to Scikit Learn. I already separated the labels, so I would just convert the dataframe into a numpy matrix. Why I did not handle True and False some may ask. \n",
      "It turns out that booleans `False` and `True` are actually subclasses of integers in Python. If you try to do `False + True` in a Python REPL, you would get 1. That is because `True` is represented as 1 and `False` is represented as 0. Numpy uses this boolean information to convert the booleans into matrix. I just need to coerce `astype(np.float)` when I convert the pandas dataframe to numpy matrix, which I will do exactly as next step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([[  16.  ,  128.  ,    0.  , ...,    3.  ,    2.7 ,    1.  ],\n",
        "       [  35.  ,  107.  ,    0.  , ...,    3.  ,    3.7 ,    1.  ],\n",
        "       [  31.  ,  137.  ,    0.  , ...,    5.  ,    3.29,    0.  ],\n",
        "       ..., \n",
        "       [  39.  ,   28.  ,    0.  , ...,    6.  ,    3.81,    2.  ],\n",
        "       [   6.  ,  184.  ,    1.  , ...,   10.  ,    1.35,    2.  ],\n",
        "       [  42.  ,   74.  ,    0.  , ...,    4.  ,    3.7 ,    0.  ]])"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(3333, 18)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I have 3333 instances and has 18 dimension feature vectors for each instances."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the features have quite different value ranges and some of them are discrete and some of them take continuous values, I \n",
      "need to scale them first. Removing mean and dividing the standard deviation of features respectively. Generally, this is one of the most commonly used preprocessing step. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scaler = preprocessing.StandardScaler()\n",
      "X = scaler.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "array([[-0.6786493 ,  0.67648946, -0.32758048, ..., -0.60119509,\n",
        "        -0.0856905 , -0.42793202],\n",
        "       [ 0.6031696 ,  0.14906505, -0.32758048, ..., -0.60119509,\n",
        "         1.2411686 , -0.42793202],\n",
        "       [ 0.33331299,  0.9025285 , -0.32758048, ...,  0.21153386,\n",
        "         0.69715637, -1.1882185 ],\n",
        "       ..., \n",
        "       [ 0.87302621, -1.83505538, -0.32758048, ...,  0.61789834,\n",
        "         1.3871231 ,  0.33235445],\n",
        "       [-1.35329082,  2.08295458,  3.05268496, ...,  2.24335625,\n",
        "        -1.87695028,  0.33235445],\n",
        "       [ 1.07541867, -0.67974475, -0.32758048, ..., -0.19483061,\n",
        "         1.2411686 , -1.1882185 ]])"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After preprocessed `X`, I could be sure that all of the features are in in more or less in the same range. However, one may think twice if the features are not nearly uniformed distribution(for example if a variable is categorical), you may do a different preprocessing as preprocessing makes the categorical variables like continuous variables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stratified_cv(X, y, clf_class, shuffle=True, n_folds=10, **kwargs):\n",
      "    stratified_k_fold = cross_validation.StratifiedKFold(y, n_folds=n_folds, shuffle=shuffle)\n",
      "    y_pred = y.copy()\n",
      "    for ii, jj in stratified_k_fold:\n",
      "        X_train, X_test = X[ii], X[jj]\n",
      "        y_train = y[ii]\n",
      "        clf = clf_class(**kwargs)\n",
      "        clf.fit(X_train,y_train)\n",
      "        y_pred[jj] = clf.predict(X_test)\n",
      "    return y_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I am using Stratified K Fold because there the classes are unbalanced. I do not want any folds to have only 1 particular class or even 1 class dominating the other one as it may create a bias in that particular fold. Stratification makes sure that the percentage of samples for each class is similar across folds(if not same). If you do not have an unbalanced problem, `KFold` would work fine."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Passive Aggressive Classifier: {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, linear_model.PassiveAggressiveClassifier))))\n",
      "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))))\n",
      "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, svm.SVC))))\n",
      "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, ensemble.RandomForestClassifier))))\n",
      "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, neighbors.KNeighborsClassifier))))\n",
      "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, linear_model.LogisticRegression))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Passive Aggressive Classifier: 0.82\n",
        "Gradient Boosting Classifier:  0.95"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Support vector machine(SVM):   0.92"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Random Forest Classifier:      0.95"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "K Nearest Neighbor Classifier: 0.89"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Logistic Regression:           0.86"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scores seem very good, but if I predicted all of the labels 0, what would I get as accuracy rate? Let's see."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Dump Classifier: {:.2f}'.format(metrics.accuracy_score(y, [0 for ii in y.tolist()])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dump Classifier: 0.86\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is because again unbalanced dataset problem. Since one class is 6 times of other class. I could get fairly accurate prediction if I predict the common class. What I need to look at as a metric if the classifier is doing well is to see the errors over the classes. Confusion matrices give a perfect way to see the distribution, which I will exactly use as a next step."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Confusion Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you have an unbalanced dataset problem or if you care accuracy of one class over other(maybe false-positives are not so bad for you but you definitely do not want any false negatives), then you could display the class accuracies in confusion matrices.\n",
      "Very common example of class preference of one to another is in medical applications. If you are trying to predict if a patient has cancer, you could get away with some false positives(patients that do not have cancer but are told to have cancers). However, you __never ever__ want to tell a patient that has cancer that she does not have cancer(false negative). In this type of problems, not only you should be accurate but also you cannot be wrong in ne of the squares in the confusion matrix."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Human problems cannot be solved by minimizing least squares error.  \n",
      "\n",
      "Drew Conway"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Luckily, scikit learn provides a 2-D matrix for the confusion matrix under metrics submodule, which I will use to build confusion matrices for visualization(heatmap). "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> If you are missing one tool in your machine learning workflow, search for if Scikit-Learn has an implementation. Chances are, it has."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, the heatmap in matplotlib implementation does not allow to print the matrix on top of the heatmap with default options. Heatmap is good in order to get a sense of where the classifier is not doing well, but not so good if you get numbers(you need to print out the confusion matrix separately). In order to combine both heatmap and also the numbers, I will use `seaborn`(an excellent statistical visualization library for Python) where it provides even advanced formatting for the numbers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pass_agg_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.PassiveAggressiveClassifier))\n",
      "grad_ens_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))\n",
      "decision_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, tree.DecisionTreeClassifier))\n",
      "ridge_clf_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.RidgeClassifier))\n",
      "svm_svc_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, svm.SVC))\n",
      "random_forest_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, ensemble.RandomForestClassifier))\n",
      "k_neighbors_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, neighbors.KNeighborsClassifier))\n",
      "logistic_reg_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, linear_model.LogisticRegression))\n",
      "dumb_conf_matrix = metrics.confusion_matrix(y, [0 for ii in y.tolist()]); # ignore the warning as they are all 0\n",
      "\n",
      "conf_matrix = {\n",
      "                1: {\n",
      "                    'matrix': pass_agg_conf_matrix,\n",
      "                    'title': 'Passive Aggressive',\n",
      "                   },\n",
      "                2: {\n",
      "                    'matrix': grad_ens_conf_matrix,\n",
      "                    'title': 'Gradient Boosting',\n",
      "                   },\n",
      "                3: {\n",
      "                    'matrix': decision_conf_matrix,\n",
      "                    'title': 'Decision Tree',\n",
      "                   },\n",
      "                4: {\n",
      "                    'matrix': ridge_clf_conf_matrix,\n",
      "                    'title': 'Ridge',\n",
      "                   },\n",
      "                5: {\n",
      "                    'matrix': svm_svc_conf_matrix,\n",
      "                    'title': 'Support Vector Machine',\n",
      "                   },\n",
      "                6: {\n",
      "                    'matrix': random_forest_conf_matrix,\n",
      "                    'title': 'Random Forest',\n",
      "                   },\n",
      "                7: {\n",
      "                    'matrix': k_neighbors_conf_matrix,\n",
      "                    'title': 'K Nearest Neighbors',\n",
      "                   },\n",
      "                8: {\n",
      "                    'matrix': logistic_reg_conf_matrix,\n",
      "                    'title': 'Logistic Regression',\n",
      "                   },\n",
      "                9: {\n",
      "                    'matrix': dumb_conf_matrix,\n",
      "                    'title': 'Dumb',\n",
      "                   },\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fix, ax = plt.subplots(figsize=(16, 12))\n",
      "plt.suptitle('Confusion Matrix of Various Classifiers')\n",
      "for ii, values in conf_matrix.items():\n",
      "    matrix = values['matrix']\n",
      "    title = values['title']\n",
      "    plt.subplot(3, 3, ii) # starts from 1\n",
      "    plt.title(title);\n",
      "    sns.heatmap(matrix, annot=True,  fmt='');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'heatmap'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-32-da43c6d34142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# starts from 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'heatmap'"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAn0AAAE2CAYAAAAUBCT/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAQIfQAAACZA6AMAADCB/wflkwHLMNF0+wAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10d39bb90>"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seaborn is kind of neat, huh? Gradient Boosting is indeed quite good."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to measure not just class distributions, but also a more abstract measures(like precision, recall or $f_1$ score), we could get those measures by using `classification_report` function under the metrics submodule. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.PassiveAggressiveClassifier))))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, svm.SVC))))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, ensemble.RandomForestClassifier))))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, neighbors.KNeighborsClassifier))))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.LogisticRegression))))\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, [0 for ii in y.tolist()]))); # ignore the warning as they are all 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Accuracy vs. Precision vs. Recall vs. F1 Score\n",
      "Accuracy, precision and recall are defined as follows respectively:\n",
      "$$ accuracy = \\frac{\\mbox{true positives + true negatives}}{\\mbox{true positives + true negatives + false positives + false negatives}} $$\n",
      "\n",
      "$$ precision = \\frac{\\mbox{true positives}}{\\mbox{true positives + false positives}} $$\n",
      "\n",
      "$$ recall = \\frac{\\mbox{true positives}}{\\mbox{true positives + false negatives}} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gradient Boosting Classifier not only is quite accurate for 0 class(not churning class) but also fairly accurate for churn class(class=1)). \n",
      "\n",
      "Generally, for unbalanced datasets, $f_1$ score could also give a fairly accurate estimation of how well the classifier is doing considering both classes as it is the harmonic mean of precision and recall.\n",
      "$$ f_1 = 2 \\cdot \\frac{pr \\cdot rc}{pr + rc} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-Learn provides a lot of [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for your own evaluation. You could also build your own $f_{beta}$ score if you want to weight precision more than recall or vice versa by using `fbeta_score` function under [metrics](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) in Scikit Learn. \n",
      "\n",
      "Search engines and generally information retrieval care more about precision than recall. A user could visit only so many webpages but when she visits first and second pages, she needs to see relevant/accurate results based on her query. Recall may not be very important for those cases as she is limited by time and recall may not be that important for the search results she\n",
      "sees."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Polynomial Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)\n",
      "polynomial_features = preprocessing.PolynomialFeatures()\n",
      "X = polynomial_features.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.PassiveAggressiveClassifier))))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, svm.SVC))))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, ensemble.RandomForestClassifier))))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, neighbors.KNeighborsClassifier))))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.LogisticRegression))))\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, [0 for ii in y.tolist()]))); # ignore the warning as they are all 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Scaled and Polynomial Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you think, when some of the features come together, they could form a much more powerful feature, or just by getting the square of the feature would be powerful feature, then Scikit-Learn has something that quite fits to your needs. Let's aasume I have `[x, y]` feature vector and I am interested in  `[1, x, y, x^2, xy, y^2]`, in the preprocessing step, I could use `PolynomialFeatures` of Scikit-Learn to build that feature matrix. If I just want to only get the interaction features(not `x^2`, then it is enough to pass `interaction_only=True` and ` include_bias=False`. If you want to get higher order Polynomial features(say nth degree), pass `degree=n` optional parameter to Polynomial Features. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df.as_matrix().astype(np.float)\n",
      "scaler = preprocessing.StandardScaler()\n",
      "X = scaler.fit_transform(X)\n",
      "polynomial_features = preprocessing.PolynomialFeatures()\n",
      "X = polynomial_features.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Passive Aggressive Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.PassiveAggressiveClassifier))))\n",
      "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))))\n",
      "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, svm.SVC))))\n",
      "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, ensemble.RandomForestClassifier))))\n",
      "print('K Nearest Neighbor Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, neighbors.KNeighborsClassifier))))\n",
      "print('Logistic Regression:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, linear_model.LogisticRegression))))\n",
      "print('Dump Classifier:\\n {}\\n'.format(metrics.classification_report(y, [0 for ii in y.tolist()]))); # ignore the warning as they are all 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regresion refers learning how to relate the input variables to output variables. This is true for classification as well with only one difference that the regression would deal with continuous output variables where classification produces discrete output variables. One can use a regressor to output discrete variables as well using basic thresholding although it is not common. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets.california_housing import fetch_california_housing\n",
      "\n",
      "california_housing = sklearn.datasets.california_housing.fetch_california_housing()\n",
      "california_housing_data = california_housing['data']\n",
      "california_housing_labels = california_housing['target']# 'target' variables\n",
      "california_housing_feature_names = california_housing['feature_names']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cross-Validation\n",
      "- Split the data 80/20 rule, test_size parameter determines the ratio of test data in the whole dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(california_housing_data,\n",
      "                                                    california_housing_labels,\n",
      "                                                    test_size=0.2,\n",
      "                                                    random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(X_train.shape)\n",
      "print(X_test.shape)\n",
      "print('Training/Test Ratio: {}'.format(X_train.shape[0] / X_test.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far so good!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Regression model parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parameters = {\n",
      "              'n_estimators': 500, \n",
      "              'max_depth': 4, \n",
      "              'min_samples_split': 1,\n",
      "              'learning_rate': 0.01, \n",
      "              'loss': 'ls'\n",
      "             }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### We will use an ensemble model to predict housing price"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import ensemble\n",
      "from sklearn import metrics\n",
      "classifier = ensemble.GradientBoostingRegressor(**parameters)\n",
      "\n",
      "classifier.fit(X_train, y_train)\n",
      "predictions = classifier.predict(X_test)\n",
      "mse = metrics.mean_squared_error(y_test, predictions)\n",
      "print('Mean Square Error: {:.3f}'.format(mse))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### This may take some time, but it will worth it I promise\n",
      "parameters = {\n",
      "              'n_estimators': 3000, \n",
      "              'max_depth': 6, \n",
      "              'learning_rate': 0.04, \n",
      "              'loss': 'huber'\n",
      "             }\n",
      "classifier = ensemble.GradientBoostingRegressor(**parameters)\n",
      "\n",
      "classifier.fit(X_train, y_train)\n",
      "predictions = classifier.predict(X_test)\n",
      "mse = metrics.mean_squared_error(y_test, predictions)\n",
      "print('Mean Squared Error: {:.3f}'.format(mse))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yep, this is much better. But instead of setting the parameters like this and then look at the mean squared error, maybe I should use another way to optimize the parameters based on mean squared eror. I will see how to do that in the next section; GridSearch to optimize the parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(16, 12))\n",
      "\n",
      "plt.scatter(range(predictions.shape[0]), predictions, label='predictions', c='#348ABD', alpha=0.4)\n",
      "plt.scatter(range(y_test.shape[0]), y_test, label='actual values', c='#A60628', alpha=0.4)\n",
      "plt.ylim([y_test.min(), predictions.max()])\n",
      "plt.xlim([0, predictions.shape[0]])\n",
      "plt.legend();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_score = [classifier.loss_(y_test, y_pred) for y_pred in classifier.staged_decision_function(X_test)]\n",
      "\n",
      "plt.figure(figsize=(16, 12))\n",
      "plt.title('Deviance');\n",
      "plt.plot(np.arange(parameters['n_estimators']) + 1, classifier.train_score_, c='#348ABD',\n",
      "         label='Training Set Deviance');\n",
      "plt.plot(np.arange(parameters['n_estimators']) + 1, test_score, c='#A60628',\n",
      "         label='Test Set Deviance');\n",
      "plt.annotate('Overfit Point', xy=(600, test_score[600]), xycoords='data',\n",
      "            xytext=(420, 0.06), textcoords='data',\n",
      "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
      "            )\n",
      "plt.legend(loc='upper right');\n",
      "plt.xlabel('Boosting Iterations');\n",
      "plt.ylabel('Deviance');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The point that red line(test) is equal to blue line(training) is our sweet spot. We want to minimize the training error but also do not overfit. Test error stays same after 600-700 boosting iterations even if training error decreases up to 3000. Training error decreases because we are overfitting after 700 boosting iterations. That is also obvious that test error slightly increasee after 700. In general, both classification and also regression, in order to prevent overfitting, one not only looks at the success rate of the classifier/regressor in the training data but also in the test data as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get Feature Importance from the classifier\n",
      "feature_importance = classifier.feature_importances_\n",
      "# Normalize The Features\n",
      "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
      "sorted_idx = np.argsort(feature_importance)\n",
      "pos = np.arange(sorted_idx.shape[0]) + .5\n",
      "plt.figure(figsize=(16, 12))\n",
      "plt.barh(pos, feature_importance[sorted_idx], align='center', color='#7A68A6')\n",
      "plt.yticks(pos, np.asanyarray(california_housing_feature_names)[sorted_idx])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.title('Variable Importance')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In here I am plotting the relative importances of the features as RF could estimate which feature could play more important role than other features. This part generally is done in explaratory data analysis part but it is nice to have at least some idea on how classifier treats the features as well. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classifiers both in terms of their architecture but also capabilities differ. If you do not know beforehand which feature is important to predict the outcome of the particular class, you may want to choose a classifier which has both feature selection capability(which could score the features based on how useful it is or minimizing least squared error like RandomForest does in this case). Some of the classifiers support class weighting(if one class is more important than the other one) through `class_weight` parameter (e.g.: Suppor Vector Classifier, SVC). When you want to choose a classifier, apart from its merits, these out of the box capabilities could also influence your thinking and make your decision about which classifier is best."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = ['MedInc', 'AveOccup', 'HouseAge', 'AveRooms',\n",
      "            ('AveOccup', 'HouseAge')]\n",
      "fig, ax = ensemble.partial_dependence.plot_partial_dependence(classifier, X_train, features, \n",
      "                                                              feature_names=california_housing_feature_names, \n",
      "                                                              figsize=(16, 12));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this part, one could also look at the partial dependence plots to see which feature influence more and also how tdoe they disribute across different feature categories."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I made an intentional mistake in this notebook. \n",
      "\n",
      "- Can you think of what might go wrong in the feature set? \n",
      "- Are features scaled? If not, can scaling help? Why?\n",
      "- What type of scaling would be useful to preprocess the targer variable(price of house)? Why?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### TODO\n",
      "- Change the `target` variable to `np.log(target)` for prediction and do a similar feature importance and deviance plot for that target variable.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}